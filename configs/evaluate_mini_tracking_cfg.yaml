# Config for naming logging directory
run_id: 'evaluation'
add_date: True

# Config for logging directory path
output_path : '/media/HDD2/students/maximilian/spatio-temporal-gnn' 

# Config to load previous model
# load_checkpoint : False # Set to true to load a previous checkpoint
# overwrite_saved_hparam: False # Set to true to if you want to use new hyperparameters(Defined in this config file) 
# ckpt_path: 'experiments/path_/to/_checkpoint.ckpt'
ckpt_path: 'experiments/05-05_18:30_train_w_LR_scheduler/model_checkpoints/epoch=17-step=48329.ckpt'

# Config for modes from the nuscenes split
# train_dataset_mode: "train"
# eval_dataset_mode: "val"
test_dataset_mode: "mini_val"

gpu_settings:
  device_type: "gpu"
  # Specify which GPUs to use
  device_id: [0] # Specifically takes the gpu associated with cuda:1 
  torch_device: 'cuda:0'

# train_params:
#   batch_size: 4
#   num_epochs: 200
#   optimizer:
#     type: Adam
#     args:
#       lr: 0.0001
#       weight_decay: 0.001

#   lr_scheduler:
#     type:
#     args:
#       step_size: 7
#       gamma: 0.5

#   num_workers: 0 # Used for dataloaders
#   save_every_epoch: True # Determines if every a checkpoint will be saved for every epoch
#   save_epoch_start: 1 # If the arg above is set to True, determines the first epoch after which we start saving ckpts
#   tensorboard: True
#   num_save_top_k: 2
#   include_custom_checkpointing : True
#   include_early_stopping : True

dataset_params:
  # Preprocessing ParamsL
  load_valid_sequence_sample_list : True
  # If the above is true then specify the path to the pickle files
  # Must be absolutepath until now
  # if mode not train or val then sequence_sample_list_train_path will be loaded into Dataset object
  sequence_sample_list_train_path : '/media/HDD2/students/maximilian/spatio-temporal-gnn/dataset/preprocess_dataset_mini_nuscenes/sequence_sample_list_mini_train.pkl'
  sequence_sample_list_val_path : '/media/HDD2/students/maximilian/spatio-temporal-gnn/dataset/preprocess_dataset_mini_nuscenes/sequence_sample_list_mini_val.pkl'

  # Dataset Processing params:
  dataset_version: 'v1.0-mini'
  dataroot : '/media/HDD2/Datasets/mini_nusc'
  is_windows_path : False

  # Filter Parameter
  # Contains a list of nuscenes class labels.
  # Only detections from these clases will be loaded for the graph
  # filterBoxes_categoryQuery: ['vehicle.car'] 
  filterBoxes_categoryQuery: ['vehicle.car'] 

  # Graph Construction Parameters
  graph_construction_params:
    spatial_knn_num_neighbors: 3
    temporal_knn_num_neighbors : 3
    spatial_shift_timeframes : 20
    
  max_frame_dist: 3 # Maximum number of frames contained in each graph sampled graph
  # Node Features
  node_feature_mode : "centers_and_time" # determines the included node features 
  # Edge Features
  edge_feature_mode : "edge_type" # determines the included edge features 
  # Edge_Label Type
  label_type: "binary" # "binary" or "multiclass"
  # Choose how Graph construction should be handled
  adapt_knn_param: False # if number of objects is below the KNN-Param then K

  # Data Augmentation Params
  augment: False # Determines whether data augmentation is performed

  

data_splits: # See nuscenes.datasplit()
  train: ['scene-0061', 'scene-0553', 'scene-0655', 'scene-0757', 'scene-0796', 'scene-1077', 'scene-1094', 'scene-1100']
  val: ['scene-0103', 'scene-0916']
  test: []

eval_params:
  # Logging / Metrics reporting params
  tensorboard: True
  save_submission: True
  use_gt: False
  tracking_threshold: 0.5
  # check_val_every_n_epoch: 9999
  # val_percent_check: 0.15 # Percentage of the entire dataset used each time that validation loss is computed
  # mot_metrics_to_log: ['mota', 'norm_mota', 'idf1', 'norm_idf1', 'num_switches', 'num_misses', 'num_false_positives', 'num_fragmentations', 'constr_sr']
  # metrics_to_log: ['loss', 'precision', 'recall', 'constr_sr']
  # log_per_seq_metrics: False
  # normalize_mot_metrics: True # Determines whether MOT results are computer via an oracle (i.e. GT labels), in order to
  #                             # normalize results. (i.e. what is the best possible MOTA we could get with a set of dets?)
  # mot_metrics_to_norm: ['mota', 'idf1']
  # best_method_criteria: 'idf1'

  # # Inference Params
  # max_dets_per_graph_seq: 40000 # Entire sequences are split into smaller subsequences of up to max_dets_per_graph_seq
  #                               # detections to avoid loading into (GPU) memory massive graphs.
  # rounding_method: exact # Determines whether an LP is used for rounding ('exact') or a greedy heuristic ('greedy')
  # solver_backend: pulp # Determines package used to solve the LP, (Gurobi requires a license, pulp does not)
  # set_pruned_edges_to_inactive: False # Determines whether pruning an edge during inference has the same effect
  #                                     # as predicting it as being non-active, or as not predicting a value for it
  #                                     # (i.e. the averaging is only computed among the times the edge is not pruned)

  # # Postprocessing parameters:
  # use_tracktor_start_ends: True
  # add_tracktor_detects: True
  # min_track_len: 2

# graph_model_params:
#   node_agg_fn: 'sum'
#   num_enc_steps: 5  # Number of message passing steps
#   num_class_steps: 3  # Number of message passing steps during feature vectors are classified (after Message Passing)
#   reattach_initial_nodes: False  # Determines whether initially encoded node feats are used during node updates
#   reattach_initial_edges: True  # Determines whether initially encoded edge feats are used during node updates

#   encoder_feats_dict:
#       edge_in_dim: 2
#       edge_fc_dims: [18, 18]
#       edge_out_dim: 2
#       node_in_dim: 4
#       #node_fc_dims: [512, 128]
#       node_fc_dims: [128]
#       node_out_dim: 4
#       dropout_p: 0
#       use_batchnorm: False
  
#   edge_model_feats_dict:
#     # Input size is 1(=node_factor) * 2 * #encoded nodes ('node_out_dim') 
#     # + 2(=edge_factor) * #encoded edges (edge_out_dim)
#     # Make sure last layer's output dimensions 
#     # equal the encoded edge feature size
#     fc_dims: [80, 2] 
#     dropout_p: 0
#     use_batchnorm: False

#   node_model_feats_dict:
#     # In size is 1(=node_factor) * encoded nodes('node_out_dim') 
#     # + 1 * encoded edges (edge_out_dim)
#     # Make sure last layer's output dimensions 
#     # equal the encoded node feature size
#     fc_dims: [56, 4]
#     dropout_p: 0
#     use_batchnorm: False

#   # only classifies edges 
#   classifier_feats_dict:
#     # edge input must be same output/last layer of edge model, edge_model_feats_dict[fc_dims][-1]
#     edge_in_dim: 2
#     edge_fc_dims: [8]
#     edge_out_dim: 1 # only 1 output for binary classification
#     dropout_p: 0
#     use_batchnorm: False
